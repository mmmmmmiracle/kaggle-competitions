{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used quite a few things from other kernels of the competetion to get staretd, just changed them so that I can understand them better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: segmentation-models in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.7 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from segmentation-models) (1.0.8)\n",
      "Requirement already satisfied: image-classifiers==0.2.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from segmentation-models) (0.2.0)\n",
      "Requirement already satisfied: scikit-image in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from segmentation-models) (0.15.0)\n",
      "Requirement already satisfied: keras>=2.2.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from segmentation-models) (2.2.5)\n",
      "Requirement already satisfied: h5py in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from keras-applications>=1.0.7->segmentation-models) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/harrison/.local/lib/python3.6/site-packages (from keras-applications>=1.0.7->segmentation-models) (1.17.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image->segmentation-models) (1.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image->segmentation-models) (3.1.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /home/harrison/.local/lib/python3.6/site-packages (from scikit-image->segmentation-models) (6.1.0)\n",
      "Requirement already satisfied: imageio>=2.0.1 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image->segmentation-models) (2.5.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image->segmentation-models) (2.3)\n",
      "Requirement already satisfied: pyyaml in /home/harrison/.local/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (5.1.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/harrison/.local/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/harrison/.local/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (2.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/harrison/.local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->segmentation-models) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /home/harrison/.local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (41.0.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: albumentations in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (0.3.2)\n",
      "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from albumentations) (0.2.6)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/harrison/.local/lib/python3.6/site-packages (from albumentations) (1.17.0)\n",
      "Requirement already satisfied: scipy in /home/harrison/.local/lib/python3.6/site-packages (from albumentations) (1.2.1)\n",
      "Requirement already satisfied: PyYAML in /home/harrison/.local/lib/python3.6/site-packages (from albumentations) (5.1.1)\n",
      "Requirement already satisfied: opencv-python-headless in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from albumentations) (4.1.0.25)\n",
      "Requirement already satisfied: six in /home/harrison/.local/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.12.0)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.15.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.3)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /home/harrison/.local/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (6.1.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.0.3)\n",
      "Requirement already satisfied: imageio>=2.0.1 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/harrison/.local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/harrison/.local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.2)\n",
      "Requirement already satisfied: setuptools in /home/harrison/.local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (41.0.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tensorflow-gpu==1.13.2 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (1.13.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/harrison/.local/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.17.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/harrison/.local/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.12.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (0.8.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.13.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (0.33.4)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/harrison/.local/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (3.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (0.8.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-gpu==1.13.2) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.2) (0.15.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.2) (3.1.1)\n",
      "Requirement already satisfied: setuptools in /home/harrison/.local/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.2) (41.0.1)\n",
      "Requirement already satisfied: h5py in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.2) (2.9.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.2) (3.0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: keras in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (2.2.5)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /home/harrison/.local/lib/python3.6/site-packages (from keras) (1.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/harrison/.local/lib/python3.6/site-packages (from keras) (1.17.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/harrison/.local/lib/python3.6/site-packages (from keras) (5.1.1)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /home/harrison/.local/lib/python3.6/site-packages (from keras) (1.2.1)\r\n",
      "Requirement already satisfied: h5py in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from keras) (2.9.0)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from keras) (1.0.8)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages (from keras) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models\n",
    "!pip install albumentations\n",
    "!pip install tensorflow-gpu==1.13.2\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import collections\n",
    "import time \n",
    "import sys\n",
    "import tqdm\n",
    "from multiprocessing import  Pool\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_on_gpu = True\n",
    "\n",
    "# Visualisation libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,    \n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomBrightnessContrast,    \n",
    "    RandomGamma    \n",
    ")\n",
    "import segmentation_models as sm\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# used from notebooks mentioned in the end\n",
    "def np_resize(img, input_shape):\n",
    "    height, width = input_shape\n",
    "    return cv2.resize(img, (width, height))\n",
    "    \n",
    "def mask2rle(img):\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle2mask(rle, input_shape):\n",
    "    width, height = input_shape[:2]\n",
    "    \n",
    "    mask = np.zeros( width*height ).astype(np.uint8)\n",
    "    \n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "        current_position += lengths[index]\n",
    "        \n",
    "    return mask.reshape(height, width).T\n",
    "\n",
    "def build_masks(rles, input_shape, reshape=None):\n",
    "    depth = len(rles)\n",
    "    if reshape is None:\n",
    "        masks = np.zeros((*input_shape, depth))\n",
    "    else:\n",
    "        masks = np.zeros((*reshape, depth))\n",
    "    \n",
    "    for i, rle in enumerate(rles):\n",
    "        if type(rle) is str:\n",
    "            if reshape is None:\n",
    "                masks[:, :, i] = rle2mask(rle, input_shape)\n",
    "            else:\n",
    "                mask = rle2mask(rle, input_shape)\n",
    "                reshaped_mask = np_resize(mask, reshape)\n",
    "                masks[:, :, i] = reshaped_mask\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def build_rles(masks, reshape=None):\n",
    "    width, height, depth = masks.shape\n",
    "    rles = []\n",
    "    \n",
    "    for i in range(depth):\n",
    "        mask = masks[:, :, i]\n",
    "        \n",
    "        if reshape:\n",
    "            mask = mask.astype(np.float32)\n",
    "            mask = np_resize(mask, reshape).astype(np.int64)\n",
    "        \n",
    "        rle = mask2rle(mask)\n",
    "        rles.append(rle)\n",
    "        \n",
    "    return rles\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "LEARNING_RATE = 2e-3\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "CHANNELS = 3\n",
    "N_CLASSES = 4\n",
    "ES_PATIENCE = 5\n",
    "RLROP_PATIENCE = 3\n",
    "DECAY_DROP = 0.5\n",
    "BACKBONE = 'efficient4'\n",
    "BATCH_SIZE=1\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.csv', 'sample_submission.csv', 'test_images', 'train_images']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'input/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22184, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{path}/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14792, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(f'{path}/sample_submission.csv')\n",
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22184 entries, 0 to 22183\n",
      "Data columns (total 2 columns):\n",
      "Image_Label      22184 non-null object\n",
      "EncodedPixels    11836 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 346.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost 10k+ null `EncodedPixels` in data. We'll add it as a flag in new column later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...\n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...\n",
       "2  0011165.jpg_Gravel                                                NaN\n",
       "3   0011165.jpg_Sugar                                                NaN\n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Image_Label` has both our label and image name, we will break them into two columns for further analysis in next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add 3 new columns: name, label and has_null_encoded_pxs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>has_null_encoded_pxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Flower</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Gravel</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>002be4f.jpg_Flower</td>\n",
       "      <td>1339279 519 1340679 519 1342079 519 1343479 51...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Flower</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002be4f.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Gravel</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>002be4f.jpg_Sugar</td>\n",
       "      <td>67495 350 68895 350 70295 350 71695 350 73095 ...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0031ae9.jpg_Fish</td>\n",
       "      <td>3510 690 4910 690 6310 690 7710 690 9110 690 1...</td>\n",
       "      <td>0031ae9.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0031ae9.jpg_Flower</td>\n",
       "      <td>2047 703 3447 703 4847 703 6247 703 7647 703 9...</td>\n",
       "      <td>0031ae9.jpg</td>\n",
       "      <td>Flower</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels  \\\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...   \n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...   \n",
       "2  0011165.jpg_Gravel                                                NaN   \n",
       "3   0011165.jpg_Sugar                                                NaN   \n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23...   \n",
       "5  002be4f.jpg_Flower  1339279 519 1340679 519 1342079 519 1343479 51...   \n",
       "6  002be4f.jpg_Gravel                                                NaN   \n",
       "7   002be4f.jpg_Sugar  67495 350 68895 350 70295 350 71695 350 73095 ...   \n",
       "8    0031ae9.jpg_Fish  3510 690 4910 690 6310 690 7710 690 9110 690 1...   \n",
       "9  0031ae9.jpg_Flower  2047 703 3447 703 4847 703 6247 703 7647 703 9...   \n",
       "\n",
       "          name   label  has_null_encoded_pxs  \n",
       "0  0011165.jpg    Fish                 False  \n",
       "1  0011165.jpg  Flower                 False  \n",
       "2  0011165.jpg  Gravel                  True  \n",
       "3  0011165.jpg   Sugar                  True  \n",
       "4  002be4f.jpg    Fish                 False  \n",
       "5  002be4f.jpg  Flower                 False  \n",
       "6  002be4f.jpg  Gravel                  True  \n",
       "7  002be4f.jpg   Sugar                 False  \n",
       "8  0031ae9.jpg    Fish                 False  \n",
       "9  0031ae9.jpg  Flower                 False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def add_features(df):\n",
    "    df['name'] = df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "    df['label'] = df['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "    df['has_null_encoded_pxs'] = df['EncodedPixels'].isnull()\n",
    "    return df\n",
    "\n",
    "train = parallelize_dataframe(train, add_features)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the exact count of null and non null EncodedPixels rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 6))\n",
    "ax = sns.countplot(y=\"has_null_encoded_pxs\", data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, remember, we've image with multiple mask. So, also check how many images have multiple masks.\n",
    "We will see this by seeing mask count groupings for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of masks</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of masks     0\n",
       "0                2  2372\n",
       "1                3  1560\n",
       "2                1  1348\n",
       "3                4   266"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[ train.has_null_encoded_pxs == False ].groupby('name').size().value_counts().reset_index().rename(columns={ 'index': 'Number of masks', '0': 'Image Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4436,) (1110,)\n"
     ]
    }
   ],
   "source": [
    "mask_count_df = train.groupby('name').agg(np.sum).reset_index()\n",
    "mask_count_df.sort_values('has_null_encoded_pxs', ascending=False, inplace=True)\n",
    "train_idx, val_idx = train_test_split(mask_count_df.index, test_size=0.2, random_state=42)\n",
    "print(train_idx.shape, val_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n",
    "                 base_path=f'{path}/train_images',\n",
    "                 batch_size=BATCH_SIZE, dim=(1400, 2100), n_channels=CHANNELS, reshape=(HEIGHT, WIDTH), \n",
    "                 n_classes=N_CLASSES, random_state=SEED, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "        self.base_path = base_path\n",
    "        self.target_df = target_df\n",
    "        self.list_IDs = list_IDs\n",
    "        self.reshape = reshape\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self.__generate_X(list_IDs_batch)\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "            y = self.__generate_y(list_IDs_batch)\n",
    "            \n",
    "            return X, y\n",
    "        \n",
    "        elif self.mode == 'predict':\n",
    "            return X\n",
    "\n",
    "        else:\n",
    "            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.seed(self.random_state)\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __generate_X(self, list_IDs_batch):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        if self.reshape is None:\n",
    "            X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        else:\n",
    "            X = np.empty((self.batch_size, *self.reshape, self.n_channels))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['name'].iloc[ID]\n",
    "            img_path = f\"{self.base_path}/{im_name}\"\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.\n",
    "            \n",
    "            if self.reshape is not None:\n",
    "                img = np_resize(img, self.reshape)\n",
    "            \n",
    "            # Store samples\n",
    "            X[i,] = img\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def __generate_y(self, list_IDs_batch):\n",
    "        if self.reshape is None:\n",
    "            y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n",
    "        else:\n",
    "            y = np.empty((self.batch_size, *self.reshape, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_batch):\n",
    "            im_name = self.df['name'].iloc[ID]\n",
    "            image_df = self.target_df[self.target_df['name'] == im_name]\n",
    "            \n",
    "            rles = image_df['EncodedPixels'].values\n",
    "            \n",
    "            if self.reshape is not None:\n",
    "                masks = build_masks(rles, input_shape=self.dim, reshape=self.reshape)\n",
    "            else:\n",
    "                masks = build_masks(rles, input_shape=self.dim)\n",
    "            \n",
    "            y[i, ] = masks\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(train_idx, df=mask_count_df, target_df=train)\n",
    " \n",
    "valid_generator = DataGenerator(val_idx, df=mask_count_df, target_df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"u-vgg16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 512, 512, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 512, 512, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 256, 256, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 256, 256, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 256, 256, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 128, 128, 128 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 128, 128, 256 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 128, 128, 256 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 128, 128, 256 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 64, 64, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 64, 64, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 64, 64, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 64, 64, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 32, 32, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 32, 32, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 32, 32, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 32, 32, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 16, 16, 512)  0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_upsample (UpSamp (None, 32, 32, 512)  0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           decoder_stage0_upsample[0][0]    \n",
      "                                                                 block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_conv1 (Conv2D)   (None, 32, 32, 256)  2359296     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_bn1 (BatchNormal (None, 32, 32, 256)  1024        decoder_stage0_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_relu1 (Activatio (None, 32, 32, 256)  0           decoder_stage0_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_conv2 (Conv2D)   (None, 32, 32, 256)  589824      decoder_stage0_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_bn2 (BatchNormal (None, 32, 32, 256)  1024        decoder_stage0_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_relu2 (Activatio (None, 32, 32, 256)  0           decoder_stage0_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_upsample (UpSamp (None, 64, 64, 256)  0           decoder_stage0_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 768)  0           decoder_stage1_upsample[0][0]    \n",
      "                                                                 block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_conv1 (Conv2D)   (None, 64, 64, 128)  884736      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_bn1 (BatchNormal (None, 64, 64, 128)  512         decoder_stage1_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_relu1 (Activatio (None, 64, 64, 128)  0           decoder_stage1_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_conv2 (Conv2D)   (None, 64, 64, 128)  147456      decoder_stage1_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_bn2 (BatchNormal (None, 64, 64, 128)  512         decoder_stage1_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_relu2 (Activatio (None, 64, 64, 128)  0           decoder_stage1_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_upsample (UpSamp (None, 128, 128, 128 0           decoder_stage1_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 384 0           decoder_stage2_upsample[0][0]    \n",
      "                                                                 block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_conv1 (Conv2D)   (None, 128, 128, 64) 221184      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_bn1 (BatchNormal (None, 128, 128, 64) 256         decoder_stage2_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_relu1 (Activatio (None, 128, 128, 64) 0           decoder_stage2_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_conv2 (Conv2D)   (None, 128, 128, 64) 36864       decoder_stage2_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_bn2 (BatchNormal (None, 128, 128, 64) 256         decoder_stage2_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_relu2 (Activatio (None, 128, 128, 64) 0           decoder_stage2_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_upsample (UpSamp (None, 256, 256, 64) 0           decoder_stage2_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256, 256, 192 0           decoder_stage3_upsample[0][0]    \n",
      "                                                                 block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_conv1 (Conv2D)   (None, 256, 256, 32) 55296       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_bn1 (BatchNormal (None, 256, 256, 32) 128         decoder_stage3_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_relu1 (Activatio (None, 256, 256, 32) 0           decoder_stage3_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_conv2 (Conv2D)   (None, 256, 256, 32) 9216        decoder_stage3_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_bn2 (BatchNormal (None, 256, 256, 32) 128         decoder_stage3_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_relu2 (Activatio (None, 256, 256, 32) 0           decoder_stage3_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_upsample (UpSamp (None, 512, 512, 32) 0           decoder_stage3_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_conv1 (Conv2D)   (None, 512, 512, 16) 4608        decoder_stage4_upsample[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_bn1 (BatchNormal (None, 512, 512, 16) 64          decoder_stage4_conv1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_relu1 (Activatio (None, 512, 512, 16) 0           decoder_stage4_bn1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_conv2 (Conv2D)   (None, 512, 512, 16) 2304        decoder_stage4_relu1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_bn2 (BatchNormal (None, 512, 512, 16) 64          decoder_stage4_conv2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_relu2 (Activatio (None, 512, 512, 16) 0           decoder_stage4_bn2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv2D)             (None, 512, 512, 4)  580         decoder_stage4_relu2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Activation)            (None, 512, 512, 4)  0           final_conv[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 19,030,020\n",
      "Trainable params: 19,028,036\n",
      "Non-trainable params: 1,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(\n",
    "           encoder_name=BACKBONE, \n",
    "           classes=N_CLASSES,\n",
    "           activation='sigmoid',\n",
    "           input_shape=(HEIGHT, WIDTH, CHANNELS))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=LEARNING_RATE), loss=binary_crossentropy, metrics=[dice_coef])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# using it from dimitreoliveira's kernel\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using `fit_generator`\n",
    "\n",
    "Note: takes 10-13 minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/harrison/anaconda3/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-11e8da9e131b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlrop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                               verbose=2).history\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              epochs=20,\n",
    "                              callbacks=[es, rlrop],\n",
    "                              verbose=2).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharex='col', figsize=(20, 12))\n",
    "\n",
    "ax.plot(history['dice_coef'], label='Train Dice coefficient')\n",
    "ax.plot(history['val_dice_coef'], label='Validation Dice coefficient')\n",
    "ax.legend(loc='best')\n",
    "ax.set_title('Dice coefficient')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Predict test images masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add same columns again in sample submission data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "sample_submission = parallelize_dataframe(sample_submission, add_features)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe for unique images only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(sample_submission['name'].unique(), columns=['name'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "test_df = []\n",
    "\n",
    "for i in range(0, test.shape[0], 500):\n",
    "    batch_idx = list(range(i, min(test.shape[0], i + 500)))\n",
    "    test_generator = DataGenerator(\n",
    "                     batch_idx,\n",
    "                     df=test,\n",
    "                     target_df=sample_submission,\n",
    "                     batch_size=1,\n",
    "                     reshape=(HEIGHT, WIDTH),\n",
    "                     dim=(350, 525),\n",
    "                     n_channels=CHANNELS,\n",
    "                     n_classes=N_CLASSES,\n",
    "                     random_state=SEED,\n",
    "                     base_path=f'{path}/test_images',\n",
    "                     mode='predict',\n",
    "                     shuffle=False)\n",
    "\n",
    "    batch_pred_masks = model.predict_generator(test_generator)\n",
    "\n",
    "    for j, b in enumerate(batch_idx):\n",
    "        filename = test['name'].iloc[b]\n",
    "        image_df = sample_submission[sample_submission['name'] == filename].copy()\n",
    "\n",
    "        pred_masks = batch_pred_masks[j, ].round().astype(int)\n",
    "        pred_rles = build_rles(pred_masks, reshape=(350, 525))\n",
    "        image_df['EncodedPixels'] = pred_rles\n",
    "        test_df.append(image_df)\n",
    "        \n",
    "submission_ready = pd.concat(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "submission_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "submission = submission_ready[['Image_Label', 'EncodedPixels']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Added submission file')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Kaggle\n",
    "# !kaggle competitions submit -c understanding_cloud_organization -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and notebook references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've looked at lot of different computer vision and segmentation notebooks for this challange to get started.\n",
    "For this compettion, I looked at notebooks of:\n",
    "1. [Artgor](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools)\n",
    "2. [dimitreoliveira](https://www.kaggle.com/dimitreoliveira/understanding-clouds-eda-and-keras-u-net)\n",
    "\n",
    "Really thankful to both and others to share their work with us.\n",
    "\n",
    "Also, for augumentation, I looked at official [sample notebook](https://github.com/albu/albumentations/blob/master/notebooks/example_kaggle_salt.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
